{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "# 使用RNN进行情绪分析\n",
    "\n",
    "在这本笔记本中，您将实施执行情绪分析的循环神经网络。使用RNN而不是feedfoward网络更准确，因为我们可以包括有关*序列*的信息。在这里，我们将使用电影评论的数据集，并附有标签。\n",
    "\n",
    "该网络的架构如下所示。\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=400px>\n",
    "\n",
    "在这里，我们将传入一个嵌入层。我们需要一个嵌入层，因为我们有成千上万的单词，所以我们需要一个比一个热编码向量更有效的输入数据表示。您应该已经从word2vec课程中看到过。实际上你可以用word2vec来训练一个嵌入，并在这里使用它。但是，只要拥有一个嵌入层，让网络学习嵌入表就可以了。\n",
    "\n",
    "从嵌入层，新的表示将被传递给LSTM单元。这些将添加到网络的重复连接，因此我们可以包括关于数据中单词序列的信息。最后，LSTM单元将在这里进入S形输出层。我们正在使用sigmoid，因为我们试图预测这个文本是否具有正面或负面的情绪。输出层只是一个单位，然后是S形激活功能。\n",
    "\n",
    "我们不关心Sigmoid输出，除了最后一个，我们可以忽略其余的。我们将从最后一步的输出和培训标签计算成本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \\nstory of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \\nhomelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "构建神经网络模型的第一步是将数据转化为正确的形式进入网络。 由于我们使用嵌入层，我们需要用一个整数对每个单词进行编码。 我们也想清理一下。\n",
    "\n",
    "您可以看到上面的评论数据的例子。 我们想摆脱那些时期。 另外，您可能会注意到，这些评论用换行符“\\ n”分隔。 为了处理这些问题，我将使用`\\ n`作为分隔符将文本分成每个评论。 然后我可以将所有的评论结合成一个大字符串。\n",
    "\n",
    "首先，我们删除所有的标点符号。 然后获取所有没有换行符的文本，并将其分成单个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "string模块中可以使用的全局变量\n",
    ">>> import string\n",
    ">>> string.digits\n",
    "'0123456789'\n",
    ">>> string.hexdigits\n",
    "'0123456789abcdefABCDEF'\n",
    ">>> string.octdigits\n",
    "'01234567'\n",
    ">>> string.letters\n",
    "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    ">>> string.lowercase\n",
    "'abcdefghijklmnopqrstuvwxyz'\n",
    ">>> string.uppercase\n",
    "'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    ">>> string.printable\n",
    "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'\n",
    ">>> string.punctuation\n",
    "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    ">>> string.whitespace\n",
    "'\\t\\n\\x0b\\x0c\\r '\n",
    ">>>\n",
    "\"\"\"\n",
    "from string import punctuation\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = all_text.split('\\n')\n",
    "\n",
    "all_text = ' '.join(reviews)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t    story of a man who has unnatural feelings for a pig  starts out with a opening scene that is a terrific example of absurd comedy  a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers  unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting  even those from the era should be turned off  the cryptic dialogue would make shakespeare seem easy to a third grader  on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond  future stars sally kirkland and frederic forrest can be seen briefly    homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter  most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets   br    br   but what if you were given a bet to live on the st'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such',\n",
       " 'as',\n",
       " 'teachers',\n",
       " 'my',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me',\n",
       " 'to',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " 's',\n",
       " 'satire',\n",
       " 'is',\n",
       " 'much',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'reality',\n",
       " 'than',\n",
       " 'is',\n",
       " 'teachers',\n",
       " 'the',\n",
       " 'scramble',\n",
       " 'to',\n",
       " 'survive',\n",
       " 'financially',\n",
       " 'the',\n",
       " 'insightful',\n",
       " 'students',\n",
       " 'who',\n",
       " 'can',\n",
       " 'see',\n",
       " 'right',\n",
       " 'through',\n",
       " 'their',\n",
       " 'pathetic',\n",
       " 'teachers',\n",
       " 'pomp',\n",
       " 'the',\n",
       " 'pettiness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'situation',\n",
       " 'all',\n",
       " 'remind',\n",
       " 'me',\n",
       " 'of',\n",
       " 'the',\n",
       " 'schools',\n",
       " 'i',\n",
       " 'knew',\n",
       " 'and',\n",
       " 'their',\n",
       " 'students',\n",
       " 'when',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'episode',\n",
       " 'in',\n",
       " 'which',\n",
       " 'a',\n",
       " 'student',\n",
       " 'repeatedly',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'burn',\n",
       " 'down',\n",
       " 'the',\n",
       " 'school',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'recalled',\n",
       " 'at',\n",
       " 'high']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码单词\n",
    "\n",
    "嵌入式查找要求我们将整数传递给我们的网络。 最简单的方法是创建将词表中的单词映射为整数的字典。 然后我们可以将每个评论转换成整数，以便将它们传递到网络中。\n",
    "\n",
    "> **练习：**现在你要用整数对单词进行编码。 构建一个将单词映射到整数的字典。 稍后我们将使用零填充输入向量，因此确保整数**从1开始，而不是0 **。\n",
    ">此外，将评论转换为整数，并将评论存储在名为“comments_ints”的新列表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 168, 8, 37, 472, 12, 59, 101, 31, 132, 154, 69, 26, 131, 642, 44, 11, 18, 530, 11, 18, 285, 23, 12, 76, 42, 326, 78, 576, 23, 59, 1394, 5, 724, 44, 8, 10, 1248, 499, 11, 20, 66284, 2, 131, 80, 10, 474, 11, 20, 215, 23, 16, 295, 19, 8, 430, 120, 1191, 1, 3348, 4, 109, 7, 7, 15, 16, 147, 12, 102, 12, 11, 20, 6, 3837, 42, 119, 5468, 72, 69, 64, 28, 1, 18, 169, 1027, 5, 1256, 16, 12, 3962, 21, 66, 8, 13, 24, 1384, 7605, 891, 12, 95, 3, 20, 653, 42, 3, 557, 17, 1, 797, 18, 169, 1027, 78, 95, 1, 20, 3, 653, 1000, 17, 2558, 295, 14, 9, 4221, 278, 13, 386, 8, 3, 20, 39, 11, 68, 114, 77, 91, 160, 32, 219, 24, 17, 1, 542, 4, 315, 298, 871, 5, 65, 1819, 1, 3292, 4, 1, 1552, 165, 6276, 11, 20, 6, 37, 1839, 12, 59, 566, 3659, 8167, 1, 764, 6276, 4, 1, 1552, 9, 1, 20, 17, 1, 146, 6276, 12, 11078, 245, 9, 4237, 59, 1, 23960, 16, 4070, 20492, 4679, 7, 7, 201, 11, 18, 3, 967, 21, 479, 1, 1384, 195, 11, 18, 61, 4759, 192, 57, 54, 11, 18, 165, 379, 45, 2, 78, 635, 2202, 34, 665, 4, 1610, 62, 6, 1, 117, 792, 4, 10290, 327, 21, 12, 11, 14, 3, 50, 531, 18, 288, 316, 93, 295, 635, 25642, 5, 1, 2217, 9, 22248, 5, 66, 11, 18, 24, 281, 24, 1417, 19, 277, 209, 2, 52, 37, 10, 65, 576, 23, 133, 12, 11, 18, 14, 24630, 32, 219, 24, 39, 1, 14596, 16, 1, 2490, 12122, 42, 1, 5968, 6, 111, 24630, 2636, 1384, 161, 23, 59, 102, 12, 2558, 61, 95, 193, 273, 5, 1083, 3659, 13, 7238, 20, 347, 12, 8, 552, 5, 95, 11, 14314, 4, 3, 20, 189, 1, 107, 273, 552, 451, 4, 12, 1391, 347, 2, 74, 52, 7, 7, 1248, 10, 115, 11, 20, 189, 11, 20, 233, 24, 29, 16, 295, 89, 23, 133, 12, 11, 20, 1849, 43, 86, 4, 862, 150, 12, 6, 1, 90, 7710, 152, 10, 139, 124, 547, 19384, 3, 18, 43, 86, 8, 47, 862, 9, 8, 1, 63, 14, 1019, 1, 410, 97, 28, 77, 128, 19, 278, 13, 386, 8, 1, 146, 318, 4, 1, 18, 285, 23, 4381, 42, 1747, 8, 14, 12, 7238, 1552, 10, 102, 31, 4, 1, 156, 577, 10916, 2, 9815, 120, 3, 471, 289, 8, 13, 24, 21524, 117, 158, 10, 139, 108, 74, 128, 158, 38, 197, 4, 96, 19, 8, 285, 23, 1, 247, 10, 139, 108, 22, 261, 160, 201, 96, 3, 967, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "#吧所有的单词计算词频\n",
    "counts = Counter(words)\n",
    "#找出所有的单词，\n",
    "vocab = sorted(counts,key=counts.get,reverse=True)\n",
    "#循环每个单词，找到单词对应的词频数据\n",
    "vocab_to_int = {word:li for li ,word in enumerate(vocab,1)}\n",
    "\n",
    "reviews_ints = []\n",
    "for each in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "\n",
    "print(reviews_ints[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码标签\n",
    "\n",
    "我们的标签是“正面”或“负面”。 要在我们的网络中使用这些标签，我们需要将它们转换为0和1。\n",
    "\n",
    "> **练习：**将标签从“正”和“负”分别转换为1和0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert labels to 1s and 0s for 'positive' and 'negative'\n",
    "labels = labels.split('\\n')\n",
    "labels = np.array([1 if each =='positive' else 0 for each in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你正确地构建了`标签'，你应该看到下一个输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "好的，这里有几个问题。 我们似乎有一个零长度的审查。 而且，我们的RNN的最大审查长度是太多的步骤。 让我们截断到200步。 对于小于200的评论，我们会用0秒。 对于超过200次的评论，我们可以将其截断为前200个字符。\n",
    "\n",
    "> **练习：**首先，从`comments_ints`列表中删除零长度的评论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_idx = [li for li ,review in enumerate(reviews_ints) if len(review)!=0 ]\n",
    "len(non_zero_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# Filter out that review with 0 length\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "labels = np.array([labels[ii] for ii in non_zero_idx])\n",
    "print (len(reviews_ints))\n",
    "print (len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **练习：**现在，创建一个包含我们传递给网络的数据的数组“features”。 数据应该来自`review_ints`，因为我们想把整数提供给网络。 每行应该是200元素长。 对于短于200个字的评论，左键为0。 也就是说，如果审查是“[最好的”，“电影”，“永远”]，“[117，18，128]”作为整数，行将看起来像`[0，0，0，.. 。，0，117，18，128] 对于超过200的评论，使用前200个字作为特征向量。\n",
    "\n",
    "这不是微不足道的，有一些方法可以做到这一点。 但是，如果您要建立自己的深入学习网络，那么您将不得不习惯于准备数据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "features = np.zeros((len(reviews_ints),seq_len),dtype=int)\n",
    "for i,row in enumerate(reviews_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您正确构建功能，它应该像下面的单元格输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 22089,   308,     6,\n",
       "            3,  1050,   207,     8,  2143,    32,     1,   171,    57,\n",
       "           15,    49,    81,  5851,    44,   382,   110,   140,    15,\n",
       "         5240,    60,   154,     9,     1,  5007,  5879,   475,    71,\n",
       "            5,   260,    12, 22089,   308,    13,  1979,     6,    74,\n",
       "         2406],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,    63,     4,     3,   125,\n",
       "           36,    47,  7540,  1397,    16,     3,  4191,   505,    45,\n",
       "           17],\n",
       "       [23935,    42, 55487,    15,   706, 17619,  3394,    47,    77,\n",
       "           35,  1820,    16,   154,    19,   114,     3,  1309,     5,\n",
       "          336,   147,    22,     1,   857,    12,    70,   281,  1169,\n",
       "          399,    36,   120,   283,    38,   169,     5,   382,   158,\n",
       "           42,  2278,    16,     1,   541,    90,    78,   102,     4,\n",
       "            1,  3253,    15,    43,     3,   408,  1069,   136,  8100,\n",
       "           44,   182,   140,    15,  3045,     1,   321,    22,  4863,\n",
       "        26460,   346,     5,  3094,  2092,     1, 19294, 18768,    42,\n",
       "         8100,    46,    33,   236,    29,   370,     5,   130,    56,\n",
       "           22,     1,  1929,     7,     7,    19,    48,    46,    21,\n",
       "           70,   344,     3,  2099,     5,   407,    22,     1,  1929,\n",
       "           16],\n",
       "       [ 4514,   505,    15,     3,  3346,   162,  8440,  1652,     6,\n",
       "         4833,    56,    17,  4513,  5654,   140, 11937,     5,   997,\n",
       "         4937,  2934,  4502,   566,  1199,    36,     6,  1520,    96,\n",
       "            3,   744,     4, 26527,    13,     5,    27,  3473,     9,\n",
       "        10754,     4,     8,   111,  3016,     5,     1,  1027,    15,\n",
       "            3,  4390,    82,    22,  2049,     6,  4502,   538,  2775,\n",
       "         7114, 46239,    41,   463,     1,  8440, 53277,   302,   123,\n",
       "           15,  4229,    19,  1669,   922,     1,  1652,     6,  6195,\n",
       "        20064,    34,     1,   980,  1759, 23604,   646, 25271,    27,\n",
       "          106, 11967,    13, 14282, 15503, 18413,  2465,   466, 21480,\n",
       "           36,  3275,     1,  6424,  1020,    45,    17,  2703,  2500,\n",
       "           33],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,   520,   119,   113,    34,\n",
       "        16911,  1816,  3746,   117,   885, 21111,   721,    10,    28,\n",
       "          124,   108,     2,   115,   137,     9,  1624,  7719,    26,\n",
       "          330,     5,   590,     1,  6165,    22,   386,     6,     3,\n",
       "          349,    15,    50,    15,   231,     9,  7547, 11468,     1,\n",
       "          191,    22,  9077,     6,    82,   880,   101,   111,  3590,\n",
       "            4],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           11,    20,  3660,   141,    10,   422,    23,   272,    60,\n",
       "         4357,    22,    32,    84,  3305,    22,     1,   172,     4,\n",
       "            1,   952,   506,    11,  4994,  5397,     5,   574,     4,\n",
       "         1154,    54,    53,  5333,     1,   261,    17,    41,   952,\n",
       "          125,    59,     1,   712,   137,   379,   627,    15,   111,\n",
       "         1512],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,    11,     6,   692,     1,    90,\n",
       "         2157,    20, 11831,     1,  2822,  5237,   249,    92,  3006,\n",
       "            8,   126,    24,   201,     3,   803,   634,     4, 23935,\n",
       "         1002],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,   786,   295,    10,   122,    11,     6,   418,\n",
       "            5,    29,    35,   482,    20,    19,  1282,    33,   142,\n",
       "           28,  2659,    45,  1844,    32,     1,  2788,    37,    78,\n",
       "           97,  2444,    67,  3978,    45,     2,    24,   105,   256,\n",
       "            1,   134,  1572,     2, 12516,   452,    14,   319,    11,\n",
       "           63,     6,    98,  1323,     5,   105,     1,  3776,     4,\n",
       "            3],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,    11,     6,\n",
       "           24,     1,   779,  3687,  2822,    20,     8,    14,    74,\n",
       "          325,  2739,    73,    90,     4,    27,    99,     2,   165,\n",
       "           68],\n",
       "       [   54,    10,    14,   116,    60,   798,   552,    71,   364,\n",
       "            5,     1,   730,     5,    66,  8069,     8,    14,    30,\n",
       "            4,   109,    99,    10,   293,    17,    60,   798,    19,\n",
       "           11,    14,     1,    64,    30,    69,  2506,    45,     4,\n",
       "          234,    93,    10,    68,   114,   108,  8069,   363,    43,\n",
       "         1009,     2,    10,    97,    28,  1432,    45,     1,   357,\n",
       "            4,    60,   110,   205,     8,    48,     3,  1930, 10952,\n",
       "            2,  2130,   354,   412,     4,    13,  6664,     2,  2976,\n",
       "         5193,  2126,  1368,     6,    30,     4,    60,   502,   876,\n",
       "           19,  8069,     6,    34,   227,     1,   247,   412,     4,\n",
       "          582,     4,    27,   599,     9,     1, 13985,   396,     4,\n",
       "        14113]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10,:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的数据形状很好，我们将其分为培训，验证和测试集。\n",
    "\n",
    "> **练习：**在这里创建训练，验证和测试集。 您需要为功能和标签创建集合，例如`train_x`和`train_y`。 定义一个分数分数，“split_frac”作为保留在训练集中的数据的一部分。 通常设定为0.8或0.9。 剩下的数据将被分成两半，以创建验证和测试数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "split_index = int(len(features)*split_frac)\n",
    "train_x, val_x = features[:split_index],features[split_index:]\n",
    "train_y, val_y = labels[:split_index],labels[split_index:]\n",
    "\n",
    "test_index = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_index],val_x[test_index:]\n",
    "val_y, test_y = val_y[:test_index],val_y[test_index:]\n",
    "\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具有0.8,0.1,0.1的训练，验证和文本分数，最终形状应如下所示：\n",
    "```\n",
    "                    Feature Shapes:\n",
    "Train set: \t\t (20000, 200) \n",
    "Validation set: \t(2500, 200) \n",
    "Test set: \t\t  (2500, 200)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立图表\n",
    "\n",
    "在这里，我们将构建图。 首先，定义超参数。\n",
    "\n",
    "*`lstm_size`：LSTM单元格中隐藏层中的单元数。 通常更大的是更好的性能明智。 常用值为128,256,512等\n",
    "*`lstm_layers`：网络中的LSTM层数。 我会从1开始，然后添加更多，如果我不适合。\n",
    "*`batch_size`：在一个培训通行证中提供网络的评论数量。 通常，这应该设置为尽可能高，没有内存不足。\n",
    "*“learning_rate”：学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于网络本身，我们将通过我们的200个元素长的评估向量。 每批将是“batch_size”向量。 我们还将在LSTM层上使用dropout，所以我们将为保留概率创建一个占位符。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **练习：**创建`inputs_`，`labels_`，并使用`tf.placeholder`退出`keep_prob`占位符。 `labels_`需要二维以后才能使用某些功能。 因为`keep_prob`是一个标量（一个0维张量），所以你不应该为`tf.placeholder`提供一个大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32,[None,None],name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32,[None,None],name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 嵌入\n",
    "\n",
    "现在我们将添加一个嵌入层。我们需要这样做，因为我们的词汇中有74000个单词。在这里对我们的课程进行一次热编码是非常低效的。你应该记住从word2vec课程处理这个问题。而不是单热编码，我们可以有一个嵌入层，并将该层用作查找表。您可以使用word2vec训练一个嵌入层，然后将其加载到此处。但是，只需创建一个新层，让网络学习权重就可以了。\n",
    "\n",
    "> **练习：**将嵌入式查找矩阵创建为“tf.Variable”。使用该嵌入矩阵获取嵌入的向量以[`tf.nn.embedding_lookup`]（https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup ）传递给LSTM单元格。该函数采用嵌入矩阵和输入张量，如审查向量。然后，它将返回另一个与嵌入向量的张量。因此，如果嵌入层有200个单位，则函数将返回尺寸[batch_size，200]的张量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = 300 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size),-1,1))\n",
    "    embed = tf.nn.embedding_lookup(embedding,inputs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LSTM单元格\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=400px>\n",
    "\n",
    "接下来，我们将创建我们的LSTM单元格，用于经常性网络（[TensorFlow文档]（https://www.tensorflow.org/api_docs/python/tf/contrib/rnn ））。这里我们只是定义单元格的样子。这不是实际构建图，只是在图中定义我们想要的单元格的类型。\n",
    "\n",
    "要为图形创建一个基本的LSTM单元格，您需要使用`tf.contrib.rnn.BasicLSTMCell`。查看功能文档：\n",
    "\n",
    "```\n",
    "tf.contrib.rnn.BasicLSTMCell（num_units，forget_bias = 1.0，input_size = None，state_is_tuple = True，activation = <function tanh at 0x109f1ef28>）\n",
    "```\n",
    "\n",
    "您可以看到它在该代码中使用称为“num_units”的参数，单元格中的单位数，称为“lstm_size”。那么，你可以写一些类似的东西\n",
    "\n",
    "```\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell（num_units）\n",
    "```\n",
    "\n",
    "用“num_units”创建一个LSTM单元格。接下来，您可以使用`tf.contrib.rnn.DropoutWrapper`向单元格添加退出。这只是将单元格包装在另一个单元格中，但是将输出和/或输出添加到输出中。这是一个非常方便的方式，使您的网络更好，几乎没有任何努力！所以你会做类似的事情\n",
    "\n",
    "```\n",
    "drop = tf.contrib.rnn.DropoutWrapper（cell，output_keep_prob = keep_prob）\n",
    "```\n",
    "\n",
    "大多数情况下，您的网络将具有更好的性能，更多的层。这是深入学习的魔力，增加更多的层次可以让网络学习真正复杂的关系。再次，有一个简单的方式来创建具有`tf.contrib.rnn.MultiRNNCell`的多层LSTM单元格：\n",
    "\n",
    "```\n",
    "cell = tf.contrib.rnn.MultiRNNCell（[drop] * lstm_layers）\n",
    "```\n",
    "\n",
    "这里，`[drop] * lstm_layers`创建一个长度为lstm_layers的单元格列表（`drop`）。 “MultiRNNCell”包装器将其构建到多个RNN单元格中，一个用于列表中的每个单元格。\n",
    "\n",
    "因此，您在网络中使用的最后一个单元格实际上是多个（或只有一个）具有删除的LSTM单元格。但是，从建筑的角度来看，这一切都是一样的，只是一个更复杂的单元格图形。\n",
    "\n",
    "> **练习：**下面，使用`tf.contrib.rnn.BasicLSTMCell`创建一个LSTM单元格。然后，用`tf.contrib.rnn.DropoutWrapper`添加它。最后，使用`tf.contrib.rnn.MultiRNNCell`创建多个LSTM图层。\n",
    "\n",
    "这是一个关于建立RNN的教程（https://www.tensorflow.org/tutorials/recurrent ），将帮助您。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop]*lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN forward pass\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=400px>\n",
    "\n",
    "\n",
    "现在我们需要通过RNN节点实际运行数据。您可以使用[`tf.nn.dynamic_rnn`]（https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn）来执行此操作。您将通过您创建的RNN单元格（例如，我们的多层次LSTM“单元格”）以及对网络的输入。\n",
    "\n",
    "```\n",
    "outputs，final_state = tf.nn.dynamic_rnn（cell，inputs，initial_state = initial_state）\n",
    "```\n",
    "\n",
    "以上我创建了一个初始状态`initial_state`，传递给RNN。这是在连续的时间步长中在隐藏层之间传递的单元格状态。 `tf.nn.dynamic_rnn`为我们照顾大部分的工作。我们将我们的单元格和输入信息传递到单元格，然后它对我们进行展开和其他一切。它返回每个时间步的输出和隐藏层的final_state。\n",
    "\n",
    "> **练习：**使用`tf.nn.dynamic_rnn`添加RNN中的前进路径。记住，我们实际上是从嵌入层“embed”传递向量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell,embed,initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出\n",
    "\n",
    "我们只关心最终输出，我们将使用它作为我们的情绪预测。 所以我们需要用`outputs [:, -1]`获取最后一个输出，计算出来的成本和`labels_`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证准确性\n",
    "\n",
    "这里我们可以添加几个节点来计算我们将在验证过程中使用的准确度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批处理\n",
    "\n",
    "这是从我们的数据返回批次的一个简单的功能。 首先，它会删除数据，使我们只有完整的批次。 然后，它遍历“x”和“y”数组，并从大小为[[batch_size]]的数组中返回分片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "以下是典型的训练代码。 如果你想自己做这个，请随意删除所有这些代码，并自己实现。 在运行此操作之前，请确保“checkpoints”目录存在。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 5 Train loss: 0.237\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 0.244\n",
      "Epoch: 0/10 Iteration: 15 Train loss: 0.211\n",
      "Epoch: 0/10 Iteration: 20 Train loss: 0.232\n",
      "Epoch: 0/10 Iteration: 25 Train loss: 0.229\n",
      "Val acc: 0.613\n",
      "Epoch: 0/10 Iteration: 30 Train loss: 0.232\n",
      "Epoch: 0/10 Iteration: 35 Train loss: 0.221\n",
      "Epoch: 0/10 Iteration: 40 Train loss: 0.200\n",
      "Epoch: 1/10 Iteration: 45 Train loss: 0.160\n",
      "Epoch: 1/10 Iteration: 50 Train loss: 0.179\n",
      "Val acc: 0.770\n",
      "Epoch: 1/10 Iteration: 55 Train loss: 0.136\n",
      "Epoch: 1/10 Iteration: 60 Train loss: 0.176\n",
      "Epoch: 1/10 Iteration: 65 Train loss: 0.157\n",
      "Epoch: 1/10 Iteration: 70 Train loss: 0.148\n",
      "Epoch: 1/10 Iteration: 75 Train loss: 0.118\n",
      "Val acc: 0.764\n",
      "Epoch: 1/10 Iteration: 80 Train loss: 0.117\n",
      "Epoch: 2/10 Iteration: 85 Train loss: 0.133\n",
      "Epoch: 2/10 Iteration: 90 Train loss: 0.129\n",
      "Epoch: 2/10 Iteration: 95 Train loss: 0.104\n",
      "Epoch: 2/10 Iteration: 100 Train loss: 0.095\n",
      "Val acc: 0.796\n",
      "Epoch: 2/10 Iteration: 105 Train loss: 0.116\n",
      "Epoch: 2/10 Iteration: 110 Train loss: 0.154\n",
      "Epoch: 2/10 Iteration: 115 Train loss: 0.154\n",
      "Epoch: 2/10 Iteration: 120 Train loss: 0.228\n",
      "Epoch: 3/10 Iteration: 125 Train loss: 0.202\n",
      "Val acc: 0.770\n",
      "Epoch: 3/10 Iteration: 130 Train loss: 0.157\n",
      "Epoch: 3/10 Iteration: 135 Train loss: 0.093\n",
      "Epoch: 3/10 Iteration: 140 Train loss: 0.093\n",
      "Epoch: 3/10 Iteration: 145 Train loss: 0.083\n",
      "Epoch: 3/10 Iteration: 150 Train loss: 0.090\n",
      "Val acc: 0.781\n",
      "Epoch: 3/10 Iteration: 155 Train loss: 0.106\n",
      "Epoch: 3/10 Iteration: 160 Train loss: 0.125\n",
      "Epoch: 4/10 Iteration: 165 Train loss: 0.200\n",
      "Epoch: 4/10 Iteration: 170 Train loss: 0.399\n",
      "Epoch: 4/10 Iteration: 175 Train loss: 0.436\n",
      "Val acc: 0.521\n",
      "Epoch: 4/10 Iteration: 180 Train loss: 0.281\n",
      "Epoch: 4/10 Iteration: 185 Train loss: 0.226\n",
      "Epoch: 4/10 Iteration: 190 Train loss: 0.234\n",
      "Epoch: 4/10 Iteration: 195 Train loss: 0.192\n",
      "Epoch: 4/10 Iteration: 200 Train loss: 0.189\n",
      "Val acc: 0.724\n",
      "Epoch: 5/10 Iteration: 205 Train loss: 0.141\n",
      "Epoch: 5/10 Iteration: 210 Train loss: 0.135\n",
      "Epoch: 5/10 Iteration: 215 Train loss: 0.099\n",
      "Epoch: 5/10 Iteration: 220 Train loss: 0.107\n",
      "Epoch: 5/10 Iteration: 225 Train loss: 0.133\n",
      "Val acc: 0.733\n",
      "Epoch: 5/10 Iteration: 230 Train loss: 0.130\n",
      "Epoch: 5/10 Iteration: 235 Train loss: 0.097\n",
      "Epoch: 5/10 Iteration: 240 Train loss: 0.116\n",
      "Epoch: 6/10 Iteration: 245 Train loss: 0.071\n",
      "Epoch: 6/10 Iteration: 250 Train loss: 0.097\n",
      "Val acc: 0.814\n",
      "Epoch: 6/10 Iteration: 255 Train loss: 0.082\n",
      "Epoch: 6/10 Iteration: 260 Train loss: 0.086\n",
      "Epoch: 6/10 Iteration: 265 Train loss: 0.058\n",
      "Epoch: 6/10 Iteration: 270 Train loss: 0.092\n",
      "Epoch: 6/10 Iteration: 275 Train loss: 0.079\n",
      "Val acc: 0.816\n",
      "Epoch: 6/10 Iteration: 280 Train loss: 0.095\n",
      "Epoch: 7/10 Iteration: 285 Train loss: 0.066\n",
      "Epoch: 7/10 Iteration: 290 Train loss: 0.092\n",
      "Epoch: 7/10 Iteration: 295 Train loss: 0.079\n",
      "Epoch: 7/10 Iteration: 300 Train loss: 0.085\n",
      "Val acc: 0.818\n",
      "Epoch: 7/10 Iteration: 305 Train loss: 0.062\n",
      "Epoch: 7/10 Iteration: 310 Train loss: 0.091\n",
      "Epoch: 7/10 Iteration: 315 Train loss: 0.080\n",
      "Epoch: 7/10 Iteration: 320 Train loss: 0.094\n",
      "Epoch: 8/10 Iteration: 325 Train loss: 0.052\n",
      "Val acc: 0.843\n",
      "Epoch: 8/10 Iteration: 330 Train loss: 0.070\n",
      "Epoch: 8/10 Iteration: 335 Train loss: 0.060\n",
      "Epoch: 8/10 Iteration: 340 Train loss: 0.079\n",
      "Epoch: 8/10 Iteration: 345 Train loss: 0.037\n",
      "Epoch: 8/10 Iteration: 350 Train loss: 0.048\n",
      "Val acc: 0.822\n",
      "Epoch: 8/10 Iteration: 355 Train loss: 0.044\n",
      "Epoch: 8/10 Iteration: 360 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 365 Train loss: 0.033\n",
      "Epoch: 9/10 Iteration: 370 Train loss: 0.058\n",
      "Epoch: 9/10 Iteration: 375 Train loss: 0.033\n",
      "Val acc: 0.857\n",
      "Epoch: 9/10 Iteration: 380 Train loss: 0.051\n",
      "Epoch: 9/10 Iteration: 385 Train loss: 0.035\n",
      "Epoch: 9/10 Iteration: 390 Train loss: 0.020\n",
      "Epoch: 9/10 Iteration: 395 Train loss: 0.014\n",
      "Epoch: 9/10 Iteration: 400 Train loss: 0.014\n",
      "Val acc: 0.821\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/sentiment.ckpt\n",
      "Test accuracy: 0.813\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
